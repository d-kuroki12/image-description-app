"""
ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ï¼ˆAzureå¯¾å¿œç‰ˆï¼‰
"""

import streamlit as st
import torch
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import gc
from config import *

# Azureç’°å¢ƒã§ã®æœ€é©åŒ–
if IS_AZURE:
    torch.set_num_threads(2)
    import os
    os.environ["TORCH_NUM_THREADS"] = "2"
    os.environ["MKL_NUM_THREADS"] = "2"
    os.environ["OMP_NUM_THREADS"] = "2"

# ===== ç”»åƒå‡¦ç†é–¢æ•° =====

def validate_image(uploaded_file):
    """ç”»åƒã®æ¤œè¨¼"""
    if uploaded_file is None:
        return False, "ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“"
    
    file_extension = uploaded_file.name.split('.')[-1].lower()
    if file_extension not in SUPPORTED_FORMATS:
        return False, f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„å½¢å¼ã§ã™ã€‚({', '.join(SUPPORTED_FORMATS)}ã®ã¿å¯¾å¿œ)"
    
    if uploaded_file.size > MAX_FILE_SIZE_MB * 1024 * 1024:
        return False, f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒ{MAX_FILE_SIZE_MB}MBã‚’è¶…ãˆã¦ã„ã¾ã™"
    
    return True, ""

def preprocess_image(image):
    """ç”»åƒã®å‰å‡¦ç†"""
    if image.mode != 'RGB':
        image = image.convert('RGB')
    
    if image.size[0] > MAX_IMAGE_SIZE[0] or image.size[1] > MAX_IMAGE_SIZE[1]:
        image.thumbnail(MAX_IMAGE_SIZE, Image.Resampling.LANCZOS)
    
    return image

def get_image_info(image, filename):
    """ç”»åƒæƒ…å ±ã®å–å¾—"""
    return {
        "ãƒ•ã‚¡ã‚¤ãƒ«å": filename,
        "ã‚µã‚¤ã‚º": f"{image.size[0]} x {image.size[1]} pixels",
        "ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ": image.format or "Unknown",
        "ã‚«ãƒ©ãƒ¼ãƒ¢ãƒ¼ãƒ‰": image.mode
    }

# ===== AIé–¢é€£é–¢æ•° =====

@st.cache_resource
def load_blip_model():
    """BLIPãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰"""
    try:
        with st.spinner("AIãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...ï¼ˆåˆå›ã®ã¿æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰"):
            st.info("ğŸ“¥ Hugging Faceã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            
            # ã‚ˆã‚Šã‚·ãƒ³ãƒ—ãƒ«ãªèª­ã¿è¾¼ã¿æ–¹æ³•
            processor = BlipProcessor.from_pretrained(
                BLIP_MODEL,
                cache_dir="./model_cache"  # ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®š
            )
            
            model = BlipForConditionalGeneration.from_pretrained(
                BLIP_MODEL,
                cache_dir="./model_cache",
                torch_dtype=torch.float32
            )
            
        st.success("âœ… AIãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        return processor, model
        
    except Exception as e:
        st.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ 
        st.markdown("### ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±")
        st.write(f"**ã‚¨ãƒ©ãƒ¼ã®è©³ç´°**: {str(e)}")
        st.write(f"**ãƒ¢ãƒ‡ãƒ«å**: {BLIP_MODEL}")
        
        # å¯¾å‡¦æ³•ã‚’è¡¨ç¤º
        st.markdown("### ğŸ’¡ å¯¾å‡¦æ³•")
        st.markdown("""
        1. **ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèª**ã—ã¦ãã ã•ã„
        2. **VPNã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯ä¸€æ™‚çš„ã«ç„¡åŠ¹**ã«ã—ã¦ãã ã•ã„
        3. **ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦å†è©¦è¡Œ**ã—ã¦ãã ã•ã„
        4. **ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿**ã—ã¦ãã ã•ã„
        """)
        
        return None, None

def translate_to_japanese(text):
    """åŸºæœ¬çš„ãªè‹±æ—¥ç¿»è¨³"""
    text_lower = text.lower()
    for eng, jp in TRANSLATION_DICT.items():
        if eng in text_lower:
            text_lower = text_lower.replace(eng, jp)
    return text_lower

def describe_image_with_blip(image, processor, model, language="ja"):
    """BLIPãƒ¢ãƒ‡ãƒ«ã§ç”»åƒã‚’èª¬æ˜ï¼ˆAzureæœ€é©åŒ–ç‰ˆï¼‰"""
    try:
        inputs = processor(image, return_tensors="pt")
        
        # Azureç’°å¢ƒã§ã®æœ€é©åŒ–è¨­å®š
        generation_kwargs = {
            "max_length": MAX_TOKENS,
            "num_beams": NUM_BEAMS,
            "early_stopping": True,
            "do_sample": False if IS_AZURE else True,  # Azureã§ã¯æ±ºå®šè«–çš„ç”Ÿæˆ
        }
        
        with torch.no_grad():
            out = model.generate(**inputs, **generation_kwargs)
        
        description = processor.decode(out[0], skip_special_tokens=True)
        
        if language == "ja":
            description = translate_to_japanese(description)
        
        # Azureç’°å¢ƒã§ã®ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if IS_AZURE:
            del inputs, out
            gc.collect()
        
        return description
        
    except Exception as e:
        error_msg = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        if IS_AZURE:
            error_msg += "\nï¼ˆAzureç’°å¢ƒã§ã®å‡¦ç†ä¸­ï¼‰"
        return error_msg

def get_model_info():
    """ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’å–å¾—"""
    return {
        "ãƒ¢ãƒ‡ãƒ«å": "BLIP (Bootstrapping Language-Image Pre-training)",
        "é–‹ç™ºå…ƒ": "Salesforce Research",
        "ç¨®é¡": "ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆãƒ¢ãƒ‡ãƒ«",
        "å®Ÿè¡Œç’°å¢ƒ": "Azure App Service" if IS_AZURE else "ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ",
        "ç‰¹å¾´": "ç„¡æ–™ã§ä½¿ç”¨å¯èƒ½ãªã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«"
    }